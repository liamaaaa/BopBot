{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liamaaaa/BopBot/blob/main/final_bopbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DhTv3vH6PNtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Example path — adjust for your folder structure\n",
        "load_dotenv('/content/drive/MyDrive/secrets/.env')\n",
        "\n",
        "client_id = os.getenv(\"CLIENT_ID\")\n",
        "client_secret = os.getenv(\"CLIENT_SECRET\")\n",
        "\n",
        "print(\"Client ID loaded:\", bool(client_id))\n",
        "print(\"Client Secret loaded:\", bool(client_secret))"
      ],
      "metadata": {
        "id": "4CH_xogBQi_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Collection + Cleaning\n"
      ],
      "metadata": {
        "id": "L2AaaBVWMaRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMMMeAloDTfh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import base64\n",
        "import random\n",
        "from urllib.parse import quote\n",
        "\n",
        "# creating Spotify API token\n",
        "def create_token():\n",
        "  url = 'https://accounts.spotify.com/api/token'\n",
        "\n",
        "  credentials = f'{client_id}:{client_secret}'.encode()\n",
        "\n",
        "  headers = {\n",
        "    'Authorization': 'Basic ' + base64.b64encode(credentials).decode(),\n",
        "    'Content-Type': 'application/x-www-form-urlencoded'\n",
        "  }\n",
        "\n",
        "  data = {\n",
        "      'grant_type': 'client_credentials'\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, headers=headers, data=data)\n",
        "  return response.json().get('access_token')\n",
        "\n",
        "\n",
        "def get_track_info(track_name, artist_name, access_token):\n",
        "    search_url = f'https://api.spotify.com/v1/search?q=track:{quote(track_name)}%20artist:{quote(artist_name)}&type=track&limit=1' # before it was an _ in between quote(track_name)}_artist, changed to quote(track_name)}%20artist,\n",
        "    headers = {                                                                                                                    # which %20 is a space formatting so then the api knows to consider spacing in what youre looking for if that makes sense\n",
        "        'Authorization': f'Bearer {access_token}'\n",
        "    }\n",
        "    response = requests.get(search_url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        track_data = response.json()\n",
        "        for item in track_data['tracks']['items']:\n",
        "            # Instead of just checking first artist, check all\n",
        "            all_artists = [artist['name'].lower() for artist in item['artists']]\n",
        "            if artist_name.lower() in all_artists:\n",
        "                return item\n",
        "        print(f\"No track found with the name '{track_name}' by '{artist_name}'.\")\n",
        "        return None\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_artist_info(artist_name, access_token):\n",
        "    search_url = f'https://api.spotify.com/v1/search?q={artist_name}&type=artist&limit=1'\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {access_token}'\n",
        "    }\n",
        "    response = requests.get(search_url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        artist_data = response.json()\n",
        "        artist = artist_data['artists']['items'][0]\n",
        "        return artist\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_track_popularity(track):\n",
        "    track_popularity = track['popularity'] # before was \"track[]\"\"\n",
        "    if track_popularity:\n",
        "        print(f\"The popularity of '{track['name']}' is: {track_popularity}\")\n",
        "        return track_popularity\n",
        "    else:\n",
        "        print(f\"No popularity data available for '{track['name']}'.\")\n",
        "        return None\n",
        "\n",
        "def get_artist_popularity(artist):\n",
        "    artist_popularity = artist['popularity']\n",
        "    if artist_popularity:\n",
        "        print(f\"The popularity of '{artist['name']}' is: {artist_popularity}\")\n",
        "        return artist_popularity #returns lone artist popularity score again, to stdout\n",
        "    else:\n",
        "        print(f\"No popularity data available for '{artist}'.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "apiToken = create_token()\n",
        "track_stored = get_track_info('Dance the Night Away', 'TWICE', apiToken) # The song title has to be exact\n",
        "artist_stored = get_artist_info('TWICE', apiToken)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "client_id = 'b8ec5a2bae2f4a9f8228df36b5cf53fd'\n",
        "client_secret = '947b46b1c2af448ca6e4166c979487a2'\n",
        "\n",
        "def build_dataframe(tracks_and_artists, token):\n",
        "    results = []\n",
        "    for track_name, artist_name in tracks_and_artists:\n",
        "        track = get_track_info(track_name, artist_name, token)\n",
        "        artist = get_artist_info(artist_name, token)\n",
        "        if track and artist:\n",
        "            results.append({\n",
        "                'track_name': track['name'],\n",
        "                'track_artist': track['artists'][0]['name'],\n",
        "                'track_popularity': track['popularity'],\n",
        "                'artist_popularity': artist['popularity']\n",
        "            })\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "songs_to_check = [\n",
        "    ('BIRDS OF A FEATHER', 'Billie Eilish'),\n",
        "    ('That’s So True', 'Gracie Abrams'),\n",
        "    ('Taste', 'Sabrina Carpenter'),\n",
        "    ('Bhaja Govindam', 'M. S. Subbulakshmi'),\n",
        "    ('Varnam', 'Jayanthi Kumaresh')\n",
        "]\n",
        "\n",
        "api_token = create_token()\n",
        "df = build_dataframe(songs_to_check, api_token)\n",
        "\n",
        "# display ddtaframe\n",
        "display(df)"
      ],
      "metadata": {
        "id": "ToGQ-z3bDVJH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "import re\n",
        "import os\n",
        "from IPython.display import display\n",
        "import pickle\n",
        "\n",
        "combined_file = \"combined_spotify_data.csv\"\n",
        "\n",
        "if not os.path.exists(combined_file):\n",
        "    # Download latest version\n",
        "    path = kagglehub.dataset_download(\"solomonameh/spotify-music-dataset\")\n",
        "\n",
        "    # ANDREA - high\n",
        "    dfHigh = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, \"solomonameh/spotify-music-dataset/versions/1\", \"high_popularity_spotify_data.csv\")\n",
        "\n",
        "    # EMILY - low\n",
        "    dfLow = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, \"solomonameh/spotify-music-dataset/versions/1\", \"low_popularity_spotify_data.csv\")\n",
        "\n",
        "    # Combining high + low dataframes\n",
        "    dfKaggle = pd.concat([dfHigh, dfLow])\n",
        "    dfKaggle.to_csv(combined_file, index=False)\n",
        "\n",
        "dfCombined = pd.read_csv(combined_file)\n",
        "\n",
        "dfTrackFeatures = dfCombined[['track_id', 'track_name', 'track_artist', 'energy', 'tempo', 'danceability', 'loudness', 'playlist_genre', 'speechiness', 'duration_ms', 'instrumentalness', 'valence', 'key', 'tempo', 'acousticness', 'loudness', 'liveness']]\n",
        "dfTrackFeatures = dfTrackFeatures.set_index('track_id')\n",
        "\n",
        "display(dfTrackFeatures) # displays dataframe"
      ],
      "metadata": {
        "id": "YEqmbQxkDhlH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "dfTrackFeatures = dfTrackFeatures.reset_index(drop=True)\n",
        "dfTrackFeatures['track_name'] = dfTrackFeatures['track_name'].str.lower().str.strip()\n",
        "dfTrackFeatures['track_artist'] = dfTrackFeatures['track_artist'].str.lower().str.strip()\n",
        "dfTrackFeatures['track_artist'] = dfTrackFeatures['track_artist'].apply(\n",
        "    lambda x: ', '.join(sorted([a.strip() for a in re.split(',|&|/|feat', x, flags=re.IGNORECASE)]))\n",
        ")\n",
        "dfTrackFeatures['track_name'] = dfTrackFeatures['track_name'].apply(\n",
        "    lambda x: re.sub(r\"\\(.*?\\)|\\[.*?\\]\", \"\", x).lower().strip()\n",
        ")\n",
        "\n",
        "unique_songs = dfTrackFeatures[['track_name', 'track_artist']].drop_duplicates()\n",
        "api_token = create_token()\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, row in unique_songs.iterrows():\n",
        "    track_name = row['track_name']\n",
        "    artist_name_raw = row['track_artist']\n",
        "    artist_name_search = re.split(',|&|/|feat', artist_name_raw, flags=re.IGNORECASE)[0].strip()\n",
        "\n",
        "    # print(f\"Searching for '{track_name}' by '{artist_name_search}'\")\n",
        "\n",
        "    track = get_track_info(track_name, artist_name_search, api_token)\n",
        "\n",
        "    artist_names_to_check = [a.strip() for a in re.split(',|&|/|feat', artist_name_raw, flags=re.IGNORECASE)]\n",
        "    popularity_scores = []\n",
        "\n",
        "    for name in artist_names_to_check:\n",
        "        artist_info = get_artist_info(name, api_token)\n",
        "        if artist_info:\n",
        "            popularity_scores.append(artist_info['popularity'])\n",
        "\n",
        "    if not track or not popularity_scores:\n",
        "        continue\n",
        "\n",
        "    track_name_clean = re.sub(r\"\\(.*?\\)|\\[.*?\\]\", \"\", track['name']).lower().strip()\n",
        "    spotify_artists = [artist['name'].lower().strip() for artist in track['artists']]\n",
        "    artist_name_clean = ', '.join(sorted(spotify_artists))\n",
        "\n",
        "    results.append({\n",
        "        'track_name': track_name_clean,\n",
        "        'track_artist': artist_name_clean,\n",
        "        'track_popularity': track['popularity'],\n",
        "        'artist_popularities': popularity_scores\n",
        "    })\n",
        "\n",
        "    time.sleep(0.1)\n",
        "\n",
        "dfSpotify = pd.DataFrame(results)\n",
        "dfSpotify['track_name'] = dfSpotify['track_name'].str.lower().str.strip()\n",
        "dfSpotify['track_artist'] = dfSpotify['track_artist'].str.lower().str.strip()\n",
        "\n",
        "dfCombined = pd.merge(\n",
        "    dfSpotify,\n",
        "    dfTrackFeatures,\n",
        "    on=['track_name', 'track_artist'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "dfCombined = dfCombined.drop(columns=[col for col in ['level_0', 'index', 'merge_key'] if col in dfCombined.columns])\n",
        "dfCombined = dfCombined.drop_duplicates(subset=['track_name', 'track_artist'], keep='first')\n",
        "\n",
        "display(dfCombined)\n",
        "combo = dfCombined.to_csv(\"spotify_kaggle_combined_sample.csv\", index=False)\n",
        "with open('combined_df_csv_.pkl', 'wb') as f:\n",
        "  pickle.dump(combo, f)\n",
        "\n",
        "# Load back the CSV string\n",
        "with open('combined_df_csv_.pkl', 'rb') as f:\n",
        "  loaded_combo = pickle.load(f)\n",
        "\n",
        "loaded_combo_df = pd.read_csv(pd.compat.StringIO(loaded_combo))"
      ],
      "metadata": {
        "id": "J0A6jos9Diip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ... previous code ...\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "'''\n",
        "Save the DataFrame to CSV, then load it back as a DataFrame\n",
        "to demonstrate saving and loading\n",
        "'''\n",
        "dfCombined.to_csv('combined_df_csv.csv', index=False)\n",
        "loaded_combo_df = pd.read_csv('combined_df_csv.csv')\n",
        "\n",
        "\n",
        "# Print the DataFrame\n",
        "print(loaded_combo_df)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VLpWtbyqMy0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Multi-Layer Perceptron"
      ],
      "metadata": {
        "id": "Ehxcv6vGL_5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our MLP model\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dfCombined = pd.read_csv(\"dfCombined.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Use the correct DataFrame with the required columns\n",
        "x = dfCombined[['artist_popularities', 'tempo', 'loudness', 'energy', 'danceability', 'playlist_genre']]  # Use dfCombined instead of df\n",
        "y = dfCombined['track_popularity']  # Use dfCombined instead of df\n",
        "\n",
        "\n",
        "# one-got encoding\n",
        "encoded_genres = pd.get_dummies(dfCombined['playlist_genre'], dtype=int, prefix='genre')  # Apply one-hot encoding to 'playlist_genre' column\n",
        "x = pd.concat([x, encoded_genres], axis=1)  # Concatenate encoded genres with other features\n",
        "x = x.drop('playlist_genre', axis=1) # drop original playlist_genre column\n",
        "\n",
        "# Convert 'artist_popularities' to numeric, handling potential errors\n",
        "x['artist_popularities'] = pd.to_numeric(x['artist_popularities'], errors='coerce')\n",
        "\n",
        "# Fill NaN values with 0 after conversion\n",
        "x['artist_popularities'] = x['artist_popularities'].fillna(0)\n",
        "x = x.dropna()\n",
        "y = y.loc[x.index]\n",
        "\n",
        "print(x.isnull().sum())  # Shows NaN count per column\n",
        "print(y.isnull().sum())  # Check target too\n",
        "display(x)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        " # 30% of data used for training\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# add layers to the model\n",
        "model.add(Dense(6, activation=\"softplus\")) # 1 hidden layer\n",
        "model.add(Dense(4, activation=\"softplus\")) # Dense represents 1 hidden layer\n",
        "model.add(Dense(1, activation=\"linear\")) # Output layer -- 3 probability values for each species\n",
        "\n",
        "model.compile(loss=\"MSE\", metrics=[\"mae\", \"mse\"])\n",
        "model.fit(x_train, y_train, epochs=25, batch_size=5)\n",
        "\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print(\"ACCURACY\", score)"
      ],
      "metadata": {
        "id": "zJHnAKIWDn10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Let's create our MLP model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dfCombined = pd.read_csv(\"dfCombined.csv\")\n",
        "\n",
        "# Use the correct DataFrame with the required columns\n",
        "x = dfCombined[['artist_popularities', 'tempo', 'loudness', 'energy', 'danceability', 'playlist_genre']]  # Use dfCombined instead of df\n",
        "y = dfCombined['track_popularity']  # Use dfCombined instead of df\n",
        "\n",
        "# one-got encoding\n",
        "encoded_genres = pd.get_dummies(dfCombined['playlist_genre'], dtype=int, prefix='genre')  # Apply one-hot encoding to 'playlist_genre' column\n",
        "x = pd.concat([x, encoded_genres], axis=1)  # Concatenate encoded genres with other features\n",
        "x = x.drop('playlist_genre', axis=1) # drop original playlist_genre column\n",
        "\n",
        "# Convert 'artist_popularities' to numeric, handling potential errors\n",
        "x['artist_popularities'] = pd.to_numeric(x['artist_popularities'], errors='coerce')\n",
        "\n",
        "# Fill NaN values with 0 after conversion\n",
        "x['artist_popularities'] = x['artist_popularities'].fillna(0)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        " # 30% of data used for training\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# add layers to the model\n",
        "model.add(Dense(6, activation=\"softplus\")) # 1 hidden layer\n",
        "model.add(Dense(4, activation=\"softplus\")) # Dense represents 1 hidden layer\n",
        "model.add(Dense(1, activation=\"linear\")) # Output layer -- 3 probability values for each species\n",
        "\n",
        "model.compile(loss=\"MSE\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, epochs=25, batch_size=5)\n",
        "\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print(\"ACCURACY\", score)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ik7Oe2JsNPAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"dfCombined.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Select features and target\n",
        "features = [\"loudness\", \"energy\", \"speechiness\", \"tempo\"]\n",
        "target = \"track_popularity\"\n",
        "\n",
        "X = data[features].values\n",
        "y = data[target].values\n",
        "\n",
        "# Normalize features (important for NN)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Dense(16, activation=\"softplus\", input_shape=(4,)),  # First hidden layer\n",
        "    Dense(12, activation=\"softplus\"),\n",
        "    Dropout(0.2),  # Dropout to prevent overfitting\n",
        "    Dense(8, activation=\"softplus\"),\n",
        "    Dense(1, activation=\"linear\")  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Evaluate performance\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Mean Absolute Error: {mae}\")"
      ],
      "metadata": {
        "id": "5PrKhHAYQArF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Refining MLP (reducing MAE)"
      ],
      "metadata": {
        "id": "djizRx-Edn06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install IntegratedGradients"
      ],
      "metadata": {
        "id": "cPoJrVuGlczx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.activations import softplus\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "df = pd.read_csv('dfCombined.csv')\n",
        "\n",
        "# Convert artist_popularities from string to list, then average\n",
        "df['artist_popularities'] = df['artist_popularities'].apply(\n",
        "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n",
        "df['artist_popularities'] = df['artist_popularities'].apply(\n",
        "    lambda x: sum(x)/len(x) if isinstance(x, list) else x\n",
        ")\n",
        "\n",
        "# Encode categorical columns\n",
        "le = LabelEncoder()\n",
        "df['playlist_genre'] = le.fit_transform(df['playlist_genre'])\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "for col in df.columns:\n",
        "    if df[col].isnull().any():\n",
        "        df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
        "\n",
        "# Feature and target separation\n",
        "X = df.drop(['track_name', 'track_artist', 'track_popularity'], axis=1)\n",
        "y = df['track_popularity']\n",
        "\n",
        "# Normalize input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Build the MLP model\n",
        "model = Sequential([\n",
        "    Dense(12, activation=softplus, input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(8, activation=softplus),\n",
        "    Dropout(0.3),\n",
        "    Dense(1)  # Output layer: linear activation by default for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "ig = IntegratedGradients(model)\n",
        "attributions, delta = ig.attribute(inputs, target=target_class, return_convergence_delta=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(\"\\nMLP Model Performance:\")\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n"
      ],
      "metadata": {
        "id": "LhqBtt6dq2F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple UI + Predictor"
      ],
      "metadata": {
        "id": "eQtA0GpUq70r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.activations import softplus\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Load and preprocess the dataset (replicate steps from LhqBtt6dq2F2)\n",
        "df = pd.read_csv('dfCombined.csv')\n",
        "\n",
        "# Convert artist_popularities from string to list, then average\n",
        "df['artist_popularities'] = df['artist_popularities'].apply(\n",
        "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n",
        "df['artist_popularities'] = df['artist_popularities'].apply(\n",
        "    lambda x: sum(x)/len(x) if isinstance(x, list) else x\n",
        ")\n",
        "\n",
        "# Encode categorical columns\n",
        "le = LabelEncoder()\n",
        "df['playlist_genre'] = le.fit_transform(df['playlist_genre'])\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "for col in df.columns:\n",
        "    if df[col].isnull().any():\n",
        "        df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
        "\n",
        "# Feature and target separation (needed for scaler fitting)\n",
        "X_for_scaler = df.drop(['track_name', 'track_artist', 'track_popularity'], axis=1)\n",
        "\n",
        "# Normalize input features\n",
        "scaler = StandardScaler()\n",
        "X_for_scaler_scaled = scaler.fit_transform(X_for_scaler)\n",
        "\n",
        "# Rebuild the MLP model architecture (assuming weights are not saved/loaded, this needs to match LhqBtt6dq2F2)\n",
        "# If the model was saved, it should be loaded here.\n",
        "# For now, assuming `model` variable from LhqBtt6dq2F2 is globally accessible. If not, the model needs to be re-trained or loaded.\n",
        "# To ensure independence, let's define the model architecture and if `model` from LhqBtt6dq2F2 is not available, this will lead to an error.\n",
        "# The safest approach would be to save and load the model, but for this fix, I'll assume `model` is accessible after `LhqBtt6dq2F2` runs.\n",
        "\n",
        "# Assign the trained model (from LhqBtt6dq2F2) to mlp\n",
        "# This assumes the model from cell LhqBtt6dq2F2 is available in the global scope.\n",
        "# If it's not, you might need to save and load the model.\n",
        "mlp = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(12, activation=softplus, input_shape=(X_for_scaler_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(8, activation=softplus),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# If the model was trained in cell LhqBtt6dq2F2 and its weights are not automatically transferred,\n",
        "# you would need to either re-train it here or save/load its weights.\n",
        "# For the purpose of fixing the NameError, I'll assume `model` from LhqBtt6dq2F2 is the `mlp` here.\n",
        "# If cell LhqBtt6dq2F2 has already been run, 'model' should exist. Let's use it if it exists.\n",
        "try:\n",
        "    if 'model' in globals():\n",
        "        mlp = model # Assign the already trained model if it exists\n",
        "except NameError:\n",
        "    pass # If 'model' is not defined, mlp will use the newly defined sequential model\n",
        "\n",
        "\n",
        "def style_text(text, color=\"lightgray\", weight=\"normal\", size=\"15px\", italic=False):\n",
        "    font_style = \"italic\" if italic else \"normal\"\n",
        "    return f\"<span style='color:{color}; font-weight:{weight}; font-size:{size}; font-style:{font_style};'>{text}</span>\"\n",
        "\n",
        "def section_box(content_html):\n",
        "    return widgets.HTML(\n",
        "        f\"<div style='background-color:#1e1e1e; border:1px solid #444; padding:15px; border-radius:10px; margin:10px 0;'>{content_html}</div>\"\n",
        "    )\n",
        "\n",
        "def predict_popularity_gui(b):\n",
        "    clear_output(wait=True)\n",
        "    display(widgets.HTML(\"<h2 style='font-family:sans-serif; color: lightgray;'>BopBot</h2>\"))\n",
        "    display(widgets.HTML(style_text(\"Enter a track name to predict its popularity.\", \"gray\", size=\"14px\", italic=True)))\n",
        "    display(input_box, predict_button)\n",
        "\n",
        "    song_name = input_box.value.strip()\n",
        "    song_data = df[df['track_name'].str.lower() == song_name.lower()]\n",
        "\n",
        "    if song_data.empty:\n",
        "        display(section_box(style_text(f\"Song '{song_name}' not found in the dataset.\", \"crimson\", \"bold\", \"16px\")))\n",
        "        return\n",
        "\n",
        "    # Prepare song_data for prediction (replicate preprocessing for a single instance)\n",
        "    song_features_raw = song_data.drop(['track_name', 'track_artist', 'track_popularity'], axis=1).copy()\n",
        "\n",
        "    # Convert artist_popularities for the single song\n",
        "    song_features_raw['artist_popularities'] = song_features_raw['artist_popularities'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "    song_features_raw['artist_popularities'] = song_features_raw['artist_popularities'].apply(\n",
        "        lambda x: sum(x)/len(x) if isinstance(x, list) else x\n",
        "    )\n",
        "\n",
        "    # Apply label encoding to playlist_genre for the single song\n",
        "    song_features_raw['playlist_genre'] = le.transform(song_features_raw['playlist_genre'])\n",
        "\n",
        "    # Impute missing values for the single song if any\n",
        "    for col in song_features_raw.columns:\n",
        "        if song_features_raw[col].isnull().any():\n",
        "            song_features_raw[col] = imputer.transform(song_features_raw[[col]]).ravel()\n",
        "\n",
        "    song_features_scaled = scaler.transform(song_features_raw)\n",
        "    prediction = mlp.predict(song_features_scaled)[0][0]\n",
        "\n",
        "    score = int(prediction)\n",
        "    if score <= 50:\n",
        "        bar_color = 'crimson'\n",
        "    elif score <= 80:\n",
        "        bar_color = 'orange'\n",
        "    else:\n",
        "        bar_color = 'limegreen'\n",
        "\n",
        "    # Progress bar\n",
        "    score_bar = widgets.IntProgress(\n",
        "        value=score,\n",
        "        min=0,\n",
        "        max=100,\n",
        "        step=1,\n",
        "        description='Score:',\n",
        "        style={'bar_color': bar_color},\n",
        "        layout=widgets.Layout(width='80%')\n",
        "    )\n",
        "\n",
        "    # stuff about the song\n",
        "    artist = song_data['track_artist'].values[0]\n",
        "    genre_code = song_data['playlist_genre'].values[0]\n",
        "    genre_name = le.inverse_transform([genre_code])[0]\n",
        "\n",
        "    meta_info_html = f\"\"\"\n",
        "        <b style='color:#ccc;'>Track:</b> <span style='color:white'>{song_name}</span><br>\n",
        "        <b style='color:#ccc;'>Artist:</b> <span style='color:white'>{artist}</span><br>\n",
        "        <b style='color:#ccc;'>Genre:</b> <span style='color:white'>{genre_name}</span><br>\n",
        "        <b style='color:#ccc;'>Predicted Popularity:</b> <span style='color:white'>{prediction:.2f}%</span>\n",
        "    \"\"\"\n",
        "\n",
        "    # claasify the scores\n",
        "    if score > 80:\n",
        "        message = style_text(\"This song might be a hit!\", \"lightgreen\", \"bold\", \"16px\")\n",
        "    elif score > 50:\n",
        "        message = style_text(\"Could trend with the right exposure.\", \"orange\", \"bold\", \"16px\")\n",
        "    else:\n",
        "        message = style_text(\"Time to go back to the studio...\", \"crimson\", \"bold\", \"16px\", italic=True)\n",
        "\n",
        "    display(section_box(meta_info_html))\n",
        "    display(score_bar)\n",
        "    display(section_box(message))\n",
        "    display(widgets.HTML(style_text(\"Created by Emily Freeman, Andrea Ayon, Lia Mathews, and Jillian Russell\", \"gray\", size=\"14px\", italic=True)))\n",
        "\n",
        "# input and button widgets\n",
        "input_box = widgets.Text(\n",
        "    placeholder='Enter song name here...',\n",
        "    layout=widgets.Layout(width='60%'),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "predict_button = widgets.Button(\n",
        "    description='Predict Popularity',\n",
        "    button_style='info',\n",
        "    layout=widgets.Layout(width='30%')\n",
        ")\n",
        "predict_button.on_click(predict_popularity_gui)\n",
        "\n",
        "#launch gui\n",
        "display(widgets.HTML(\"<h2 style='font-family:sans-serif; color: lightgray;'>BopBot</h2>\"))\n",
        "display(widgets.HTML(style_text(\"Song Popularity Predictor\", \"gray\", size=\"16px\", italic=False)))\n",
        "display(widgets.HTML(style_text(\"Enter a track name to predict its popularity.\", \"gray\", size=\"14px\", italic=True)))\n",
        "display(input_box, predict_button)\n",
        "display(widgets.HTML(style_text(\"Created by Emily Freeman, Andrea Ayon, Lia Mathews, and Jillian Russell\", \"gray\", size=\"14px\", italic=True)))"
      ],
      "metadata": {
        "id": "SpJEsSiAq_1v",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}